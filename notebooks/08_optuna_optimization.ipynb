{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2934dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, KBinsDiscretizer, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin, clone\n",
    "import os\n",
    "import joblib\n",
    "import itertools\n",
    "import logging\n",
    "import optuna\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from src.feature_engineering import *\n",
    "from src.modeling import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbff9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "DATA_PATH = \"data/\"\n",
    "OUTPUT_PATH = \"output/\"\n",
    "BLUEPRINT_PATH = \"output/blueprint/\"\n",
    "MODEL_PATH = \"models/\"\n",
    "\n",
    "PREPROCESSING_LIST = {\n",
    "    'outlier_removal' : {\n",
    "        'enabled' : True\n",
    "    },\n",
    "    'robust_scaler'  : {\n",
    "        'enabled' : True\n",
    "    },\n",
    "    'polynomial' : {\n",
    "        'enabled' : False\n",
    "    },\n",
    "    'binning' : {\n",
    "        'enabled' : False\n",
    "    },\n",
    "    'pca' : {\n",
    "        'enabled' : True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcea78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup logging ---\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(OUTPUT_PATH, \"08_optuna_optimization.log\"),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logging.info(f\"starting optuna optimization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"reading train and test data...\")\n",
    "\n",
    "df_train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "# df_train = df_train.sample(n=10000).reset_index(drop=True)\n",
    "\n",
    "df_test = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "\n",
    "target_col = \"BeatsPerMinute\"\n",
    "feature_cols = [f for f in df_train.columns if f not in ('id', target_col)]\n",
    "\n",
    "X_train = df_train[feature_cols].copy().reset_index(drop=True)\n",
    "y_train = df_train[target_col].copy().reset_index(drop=True)\n",
    "\n",
    "logging.info(f\"X_train shape : {X_train.shape}\")\n",
    "logging.info(f\"y_train shape : {y_train.shape}\")\n",
    "\n",
    "X_test = df_test[feature_cols].copy().reset_index(drop=True)\n",
    "\n",
    "logging.info(f\"X_test shape : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"defining preprocessing combinations...\")\n",
    "\n",
    "available_steps = []\n",
    "\n",
    "if PREPROCESSING_LIST[\"outlier_removal\"][\"enabled\"]:\n",
    "    available_steps.append((\"outlier_removal\", OutlierRemoval()))\n",
    "\n",
    "if PREPROCESSING_LIST[\"robust_scaler\"][\"enabled\"]:\n",
    "    available_steps.append((\"robust_scaler\", RobustScaler()))\n",
    "\n",
    "if PREPROCESSING_LIST[\"polynomial\"][\"enabled\"]:\n",
    "    available_steps.append((\"polynomial\", PolynomialFeatures(interaction_only=False, include_bias=False, degree=2)))\n",
    "\n",
    "if PREPROCESSING_LIST[\"binning\"][\"enabled\"]:\n",
    "    available_steps.append((\"binning\", KBinsDiscretizer(encode=\"ordinal\")))\n",
    "\n",
    "# if PREPROCESSING_LIST[\"standardization\"][\"enabled\"]:\n",
    "#     available_steps.append((\"standardization\", StandardScaler()) \n",
    "\n",
    "if PREPROCESSING_LIST[\"pca\"][\"enabled\"]:\n",
    "    available_steps.append((\"pca\", PCA(n_components=0.85)))\n",
    "\n",
    "def generate_all_combinations(steps):\n",
    "    for r in range(1, len(steps) + 1):\n",
    "        for combo in itertools.combinations(steps, r):\n",
    "            yield combo\n",
    "\n",
    "preprocessing_combinations = list(generate_all_combinations(available_steps))\n",
    "\n",
    "logging.info(f\"preprocessing combination count : {len(preprocessing_combinations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c32c3",
   "metadata": {},
   "source": [
    "BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45feead",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"starting BayesianRidge optuna optimization...\")\n",
    "\n",
    "def objective(trial, preprocessing_steps, X, y, n_splits=5):\n",
    "    # Hyperparameter suggestions\n",
    "    tol = trial.suggest_float(\"tol\", 1e-8, 1e-2, log=True)\n",
    "    alpha_1 = trial.suggest_float(\"alpha_1\", 1e-9, 1e3, log=True)\n",
    "    alpha_2 = trial.suggest_float(\"alpha_2\", 1e-9, 1e3, log=True)\n",
    "    lambda_1 = trial.suggest_float(\"lambda_1\", 1e-9, 1e3, log=True)\n",
    "    lambda_2 = trial.suggest_float(\"lambda_2\", 1e-9, 1e3, log=True)\n",
    "    fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "\n",
    "    model = Pipeline(\n",
    "        preprocessing_steps + [\n",
    "            (\"regressor\", BayesianRidge(\n",
    "                tol=tol,\n",
    "                alpha_1=alpha_1,\n",
    "                alpha_2=alpha_2,\n",
    "                lambda_1=lambda_1,\n",
    "                lambda_2=lambda_2,\n",
    "                fit_intercept=fit_intercept,\n",
    "                compute_score=False,\n",
    "                verbose=False\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "model_name = \"bayesian_ridge\"\n",
    "results = []\n",
    "for combo in preprocessing_combinations:\n",
    "    steps = []\n",
    "    steps_name = []\n",
    "    for step_name, transformer in combo:\n",
    "        steps.append((step_name, transformer))\n",
    "        steps_name.append(step_name)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial:objective(trial, steps, X_train, y_train), n_trials=30)\n",
    "\n",
    "    logging.info(f\"model_name: {model_name}. steps : {','.join(steps_name)}. best params: {study.best_params}. best rmse: {study.best_value}\")\n",
    "    \n",
    "    results.append({\n",
    "        'model' : model_name,\n",
    "        'steps' : ','.join(steps_name),\n",
    "        'best_params' : study.best_params,\n",
    "        'best_rmse' : study.best_value,\n",
    "    })\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values('best_rmse', ascending=True)\n",
    "    #save temporary results to csv\n",
    "    df_results.to_csv(os.path.join(OUTPUT_PATH, f\"08_optuna_optimization_{model_name}_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f942676",
   "metadata": {},
   "source": [
    "LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"starting LGBM optuna optimization...\")\n",
    "\n",
    "def objective(trial, preprocessing_steps, X, y, n_splits=5):\n",
    "    # Hyperparameter suggestions\n",
    "    param = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 256),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"objective\": \"regression\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": 1,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "\n",
    "    model = Pipeline(\n",
    "        preprocessing_steps + [\n",
    "            (\"regressor\", LGBMRegressor(**param))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "model_name = \"lgbm\"\n",
    "results = []\n",
    "for combo in preprocessing_combinations:\n",
    "    steps = []\n",
    "    steps_name = []\n",
    "    for step_name, transformer in combo:\n",
    "        steps.append((step_name, transformer))\n",
    "        steps_name.append(step_name)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial:objective(trial, steps, X_train, y_train), n_trials=30)\n",
    "\n",
    "    logging.info(f\"model_name: {model_name}. steps : {','.join(steps_name)}. best params: {study.best_params}. best rmse: {study.best_value}\")\n",
    "    \n",
    "    results.append({\n",
    "        'model' : model_name,\n",
    "        'steps' : ','.join(steps_name),\n",
    "        'best_params' : study.best_params,\n",
    "        'best_rmse' : study.best_value,\n",
    "    })\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values('best_rmse', ascending=True)\n",
    "    #save temporary results to csv\n",
    "    df_results.to_csv(os.path.join(OUTPUT_PATH, f\"08_optuna_optimization_{model_name}_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e135b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dab46",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee736a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"starting Random Forest optuna optimization...\")\n",
    "\n",
    "def objective(trial, preprocessing_steps, X, y, n_splits=5):\n",
    "    # Hyperparameter suggestions\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.5, 0.8, 1.0]),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": 1,\n",
    "    }\n",
    "\n",
    "    model = Pipeline(\n",
    "        preprocessing_steps + [\n",
    "            (\"regressor\", RandomForestRegressor(**param))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "model_name = \"rf\"\n",
    "results = []\n",
    "for combo in preprocessing_combinations:\n",
    "    steps = []\n",
    "    steps_name = []\n",
    "    for step_name, transformer in combo:\n",
    "        steps.append((step_name, transformer))\n",
    "        steps_name.append(step_name)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial:objective(trial, steps, X_train, y_train), n_trials=30)\n",
    "\n",
    "    logging.info(f\"model_name: {model_name}. steps : {','.join(steps_name)}. best params: {study.best_params}. best rmse: {study.best_value}\")\n",
    "    \n",
    "    results.append({\n",
    "        'model' : model_name,\n",
    "        'steps' : ','.join(steps_name),\n",
    "        'best_params' : study.best_params,\n",
    "        'best_rmse' : study.best_value,\n",
    "    })\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values('best_rmse', ascending=True)\n",
    "    #save temporary results to csv\n",
    "    df_results.to_csv(os.path.join(OUTPUT_PATH, f\"08_optuna_optimization_{model_name}_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30db425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c43a19f",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"starting XGB optuna optimization...\")\n",
    "\n",
    "def objective(trial, preprocessing_steps, X, y, n_splits=5):\n",
    "    # Hyperparameter suggestions\n",
    "    param = {\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),  \n",
    "        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),   # L2 regularization\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 10.0),    # L1 regularization\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 10.0),\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": 1,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "\n",
    "    model = Pipeline(\n",
    "        preprocessing_steps + [\n",
    "            (\"regressor\", XGBRegressor(**param))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "model_name = \"xgb\"\n",
    "results = []\n",
    "for combo in preprocessing_combinations:\n",
    "    steps = []\n",
    "    steps_name = []\n",
    "    for step_name, transformer in combo:\n",
    "        steps.append((step_name, transformer))\n",
    "        steps_name.append(step_name)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial:objective(trial, steps, X_train, y_train), n_trials=30)\n",
    "\n",
    "    logging.info(f\"model_name: {model_name}. steps : {','.join(steps_name)}. best params: {study.best_params}. best rmse: {study.best_value}\")\n",
    "    \n",
    "    results.append({\n",
    "        'model' : model_name,\n",
    "        'steps' : ','.join(steps_name),\n",
    "        'best_params' : study.best_params,\n",
    "        'best_rmse' : study.best_value,\n",
    "    })\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values('best_rmse', ascending=True)\n",
    "    #save temporary results to csv\n",
    "    df_results.to_csv(os.path.join(OUTPUT_PATH, f\"08_optuna_optimization_{model_name}_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c7978",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7850c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"starting CatBoost optuna optimization...\")\n",
    "\n",
    "def objective(trial, preprocessing_steps, X, y, n_splits=5):\n",
    "    # Hyperparameter suggestions\n",
    "    param = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 500, 5000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-3, 10.0, log=True),\n",
    "        # \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 10.0),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    model = Pipeline(\n",
    "        preprocessing_steps + [\n",
    "            (\"regressor\", CatBoostRegressor(**param))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "model_name = \"catboost\"\n",
    "results = []\n",
    "for combo in preprocessing_combinations:\n",
    "    steps = []\n",
    "    steps_name = []\n",
    "    for step_name, transformer in combo:\n",
    "        steps.append((step_name, transformer))\n",
    "        steps_name.append(step_name)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial:objective(trial, steps, X_train, y_train), n_trials=30)\n",
    "\n",
    "    logging.info(f\"model_name: {model_name}. steps : {','.join(steps_name)}. best params: {study.best_params}. best rmse: {study.best_value}\")\n",
    "    \n",
    "    results.append({\n",
    "        'model' : model_name,\n",
    "        'steps' : ','.join(steps_name),\n",
    "        'best_params' : study.best_params,\n",
    "        'best_rmse' : study.best_value,\n",
    "    })\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values('best_rmse', ascending=True)\n",
    "    #save temporary results to csv\n",
    "    df_results.to_csv(os.path.join(OUTPUT_PATH, f\"08_optuna_optimization_{model_name}_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-playground-series-s5e9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
